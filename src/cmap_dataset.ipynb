{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Dataset for Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"../dataset/dataset_ca_17\"\n",
    "sensors = [\"GNDVI\", \"NDVI\", \"NDVI45\", \"OSAVI\", \"PSRI\", \"RGB\"]\n",
    "hdf5_path = os.path.join(\"../dataset/dataset_ca_17.hdf5\")\n",
    "crop_mapping = {\n",
    "    \"BARLEY\": 0,\n",
    "    \"CANOLA\": 1,\n",
    "    \"CORN\": 2,\n",
    "    \"MIXEDWOOD\": 3,\n",
    "    \"OAT\": 4,\n",
    "    \"ORCHARD\": 5,\n",
    "    \"PASTURE\": 6,\n",
    "    \"POTATO\": 7,\n",
    "    \"SOYBEAN\": 8,\n",
    "    \"SPRING_WHEAT\": 9,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse filename into metadata components.\n",
    "def parse_filename(filename):\n",
    "    # Remove extension and split by underscore\n",
    "    name, _ = os.path.splitext(filename)\n",
    "    parts = name.split('_')\n",
    "    \n",
    "    # Check if the filename meets the expected pattern\n",
    "    if len(parts) < 6 or parts[0] != \"POINT\":\n",
    "        return None\n",
    "    \n",
    "    # Extract the fields\n",
    "    point = int(parts[1])\n",
    "    date = parts[2]\n",
    "    region = parts[3]\n",
    "    sensor_type = parts[-1]\n",
    "    crop_type = \"_\".join(parts[4:-1])\n",
    "    \n",
    "    # Get the integer label using the mapping dictionary; default to -1 if not found\n",
    "    label = crop_mapping.get(crop_type, -1)\n",
    "    \n",
    "    return {\n",
    "        \"POINT\": point,\n",
    "        \"DATE\": date,\n",
    "        \"REGION\": region,\n",
    "        \"CROP_TYPE\": crop_type,\n",
    "        \"SENSOR_TYPE\": sensor_type,\n",
    "        \"LABEL\": label\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection with Completeness Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total groups collected: 14111\n"
     ]
    }
   ],
   "source": [
    "# Group records by identifier: (POINT, DATE, REGION, CROP_TYPE)\n",
    "groups = {}\n",
    "for sensor_dir in sensors:\n",
    "    sensor_path = os.path.join(base_dir, sensor_dir)\n",
    "    if not os.path.isdir(sensor_path):\n",
    "        continue\n",
    "    for root, _, files in os.walk(sensor_path):\n",
    "        for file in files:\n",
    "            # Verify file type and metadata\n",
    "            if not file.lower().endswith(('.png')):\n",
    "                continue\n",
    "            if not file.startswith(\"POINT_\"):\n",
    "                continue\n",
    "            meta = parse_filename(file)\n",
    "            if meta is None:\n",
    "                continue\n",
    "\n",
    "            # Use the common identifier tuple (POINT, DATE, REGION, CROP_TYPE)\n",
    "            identifier = (meta[\"POINT\"], meta[\"DATE\"],\n",
    "                          meta[\"REGION\"], meta[\"CROP_TYPE\"])\n",
    "\n",
    "            # Read the image file as raw bytes\n",
    "            try:\n",
    "                with open(file, 'rb') as f:\n",
    "                    image_bytes = f.read()\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Create group entry if not already present; initialize sensor columns to None\n",
    "            if identifier not in groups:\n",
    "                groups[identifier] = {\n",
    "                    \"POINT\": meta[\"POINT\"],\n",
    "                    \"DATE\": meta[\"DATE\"],\n",
    "                    \"REGION\": meta[\"REGION\"],\n",
    "                    \"CROP_TYPE\": meta[\"CROP_TYPE\"],\n",
    "                    \"LABEL\": meta[\"LABEL\"],\n",
    "                    **{sensor: None for sensor in sensors}\n",
    "                }\n",
    "\n",
    "            # Validate and store sensor data\n",
    "            if meta[\"SENSOR_TYPE\"] in sensors:\n",
    "                groups[identifier][meta[\"SENSOR_TYPE\"]] = image_bytes\n",
    "            else:\n",
    "                print(f\"Invalid sensor {meta['SENSOR_TYPE']} in {file}\")\n",
    "\n",
    "# Filter complete records with all sensors present\n",
    "complete_groups = {\n",
    "    k: v for k, v in groups.items() \n",
    "    if all(v[sensor] is not None for sensor in sensors)\n",
    "}\n",
    "\n",
    "print(f\"Complete records: {len(complete_groups)} (Original: {len(groups)})\")\n",
    "records = list(complete_groups.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset into HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving sensor images: 100%|██████████| 14111/14111 [01:01<00:00, 230.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ../dataset/dataset_ca_17.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Extract metadata columns\n",
    "points = [rec[\"POINT\"] for rec in records]\n",
    "dates = [rec[\"DATE\"] for rec in records]\n",
    "regions = [rec[\"REGION\"] for rec in records]\n",
    "crop_types = [rec[\"CROP_TYPE\"] for rec in records]\n",
    "labels = [rec[\"LABEL\"] for rec in records]\n",
    "\n",
    "# Helper function: Convert image bytes to uint8 numpy array (or empty array if None)\n",
    "\n",
    "\n",
    "def convert_image_bytes(img_bytes):\n",
    "    if img_bytes is None:\n",
    "        return np.array([], dtype='uint8')\n",
    "    return np.frombuffer(img_bytes, dtype='uint8')\n",
    "\n",
    "\n",
    "# Convert sensor images\n",
    "rgb_images = [convert_image_bytes(rec[\"RGB\"]) for rec in records]\n",
    "ndvi_images = [convert_image_bytes(rec[\"NDVI\"]) for rec in records]\n",
    "ndvi45_images = [convert_image_bytes(rec[\"NDVI45\"]) for rec in records]\n",
    "osavi_images = [convert_image_bytes(rec[\"OSAVI\"]) for rec in records]\n",
    "psri_images = [convert_image_bytes(rec[\"PSRI\"]) for rec in records]\n",
    "gndvi_images = [convert_image_bytes(rec[\"GNDVI\"]) for rec in records]\n",
    "\n",
    "with h5py.File(hdf5_path, \"w\") as hf:\n",
    "    # Create datasets for metadata\n",
    "    hf.create_dataset(\"POINT\", data=np.array(points, dtype=np.int64))\n",
    "    hf.create_dataset(\"DATE\", data=np.array(\n",
    "        dates, dtype=h5py.string_dtype(encoding='utf-8')))\n",
    "    hf.create_dataset(\"REGION\", data=np.array(\n",
    "        regions, dtype=h5py.string_dtype(encoding='utf-8')))\n",
    "    hf.create_dataset(\"CROP_TYPE\", data=np.array(\n",
    "        crop_types, dtype=h5py.string_dtype(encoding='utf-8')))\n",
    "    hf.create_dataset(\"LABEL\", data=np.array(labels, dtype=np.int64))\n",
    "\n",
    "    # Create variable-length datasets for sensor images using uint8 arrays\n",
    "    vlen_uint8 = h5py.special_dtype(vlen=np.dtype('uint8'))\n",
    "    hf.create_dataset(\"RGB\",    (len(rgb_images),),    dtype=vlen_uint8)\n",
    "    hf.create_dataset(\"NDVI\",   (len(ndvi_images),),   dtype=vlen_uint8)\n",
    "    hf.create_dataset(\"NDVI45\", (len(ndvi45_images),), dtype=vlen_uint8)\n",
    "    hf.create_dataset(\"OSAVI\",  (len(osavi_images),),  dtype=vlen_uint8)\n",
    "    hf.create_dataset(\"PSRI\",   (len(psri_images),),   dtype=vlen_uint8)\n",
    "    hf.create_dataset(\"GNDVI\",  (len(gndvi_images),),  dtype=vlen_uint8)\n",
    "\n",
    "    # Write sensor image data row by row\n",
    "    hf.create_dataset(\"RGB\", data=rgb_images, dtype=vlen_uint8)\n",
    "    hf.create_dataset(\"NDVI\", data=ndvi_images, dtype=vlen_uint8)\n",
    "    hf.create_dataset(\"NDVI45\", data=ndvi45_images, dtype=vlen_uint8)\n",
    "    hf.create_dataset(\"OSAVI\", data=osavi_images, dtype=vlen_uint8)\n",
    "    hf.create_dataset(\"PSRI\", data=psri_images, dtype=vlen_uint8)\n",
    "    hf.create_dataset(\"GNDVI\", data=gndvi_images, dtype=vlen_uint8)\n",
    "\n",
    "print(f\"Data saved to {hdf5_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify HDF5 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets in the file:\n",
      "CROP_TYPE: (14111,)\n",
      "DATE: (14111,)\n",
      "GNDVI: (14111,)\n",
      "LABEL: (14111,)\n",
      "NDVI: (14111,)\n",
      "NDVI45: (14111,)\n",
      "OSAVI: (14111,)\n",
      "POINT: (14111,)\n",
      "PSRI: (14111,)\n",
      "REGION: (14111,)\n",
      "RGB: (14111,)\n"
     ]
    }
   ],
   "source": [
    "# Open the file and print out the keys\n",
    "with h5py.File(hdf5_path, \"r\") as hf:\n",
    "    print(\"Datasets in the file:\")\n",
    "    for key in hf.keys():\n",
    "        print(f\"{key}: {hf[key].shape}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMz4qbCTf4SgqZeC50GYS41",
   "mount_file_id": "1O2z0KrJuca8mLrY2PvlYbmyV7PeSywM3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
