{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Dataset for Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"../dataset/dataset_ca_17\"\n",
    "sensor_dirs = [\"GNDVI\", \"NDVI\", \"NDVI45\", \"OSAVI\", \"PSRI\", \"RGB\"]\n",
    "hdf5_path = os.path.join(\"../dataset/dataset_ca_17.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Mapping for Crop Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_mapping = {\n",
    "    \"BARLEY\": 0,\n",
    "    \"CANOLA\": 1,\n",
    "    \"CORN\": 2,\n",
    "    \"MIXEDWOOD\": 3,\n",
    "    \"OAT\": 4,\n",
    "    \"ORCHARD\": 5,\n",
    "    \"PASTURE\": 6,\n",
    "    \"POTATO\": 7,\n",
    "    \"SOYBEAN\": 8,\n",
    "    \"SPRING_WHEAT\": 9,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse a filename of the format:\n",
    "# POINT_<point>_<date>_<region>_<crop_type>_<sensor_type>.png\n",
    "# Returns a dictionary with keys:\n",
    "# POINT, DATE, REGION, CROP_TYPE, SENSOR_TYPE, LABEL\n",
    "def parse_filename(filename):\n",
    "    # Remove extension and split by underscore\n",
    "    name, _ = os.path.splitext(filename)\n",
    "    parts = name.split('_')\n",
    "    \n",
    "    # Check if the filename meets the expected pattern\n",
    "    if len(parts) < 6 or parts[0] != \"POINT\":\n",
    "        return None\n",
    "    \n",
    "    # Extract the fields\n",
    "    point = int(parts[1])\n",
    "    date = parts[2]\n",
    "    region = parts[3]\n",
    "    sensor_type = parts[-1]\n",
    "    crop_type = \"_\".join(parts[4:-1])\n",
    "    \n",
    "    # Get the integer label using the mapping dictionary; default to -1 if not found\n",
    "    label = crop_mapping.get(crop_type, -1)\n",
    "    \n",
    "    return {\n",
    "        \"POINT\": point,\n",
    "        \"DATE\": date,\n",
    "        \"REGION\": region,\n",
    "        \"CROP_TYPE\": crop_type,\n",
    "        \"SENSOR_TYPE\": sensor_type,\n",
    "        \"LABEL\": label\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Dataset from Subdirectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total groups collected: 14111\n"
     ]
    }
   ],
   "source": [
    "# Group records by identifier: (POINT, DATE, REGION, CROP_TYPE)\n",
    "groups = {}\n",
    "for sensor_dir in sensor_dirs:\n",
    "    sensor_path = os.path.join(base_dir, sensor_dir)\n",
    "    if not os.path.isdir(sensor_path):\n",
    "        continue\n",
    "    for root, _, files in os.walk(sensor_path):\n",
    "        for file in files:\n",
    "            # Verify file type and metadata\n",
    "            if not file.lower().endswith(('.png')):\n",
    "                continue\n",
    "            if not file.startswith(\"POINT_\"):\n",
    "                continue\n",
    "            meta = parse_filename(file)\n",
    "            if meta is None:\n",
    "                continue\n",
    "            \n",
    "            # Use the common identifier tuple (POINT, DATE, REGION, CROP_TYPE)\n",
    "            identifier = (meta[\"POINT\"], meta[\"DATE\"], meta[\"REGION\"], meta[\"CROP_TYPE\"])\n",
    "            \n",
    "            # Read the image file as raw bytes\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    image_bytes = f.read()\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Create group entry if not already present; initialize sensor columns to None\n",
    "            if identifier not in groups:\n",
    "                groups[identifier] = {\n",
    "                    \"POINT\": meta[\"POINT\"],\n",
    "                    \"DATE\": meta[\"DATE\"],\n",
    "                    \"REGION\": meta[\"REGION\"],\n",
    "                    \"CROP_TYPE\": meta[\"CROP_TYPE\"],\n",
    "                    \"LABEL\": meta[\"LABEL\"],\n",
    "                    \"RGB\": None,\n",
    "                    \"NDVI\": None,\n",
    "                    \"NDVI45\": None,\n",
    "                    \"OSAVI\": None,\n",
    "                    \"PSRI\": None,\n",
    "                    \"GNDVI\": None,\n",
    "                }\n",
    "            \n",
    "            # Determine the sensor type (force uppercase for consistency)\n",
    "            sensor_type = meta[\"SENSOR_TYPE\"].upper()\n",
    "            if sensor_type in groups[identifier]:\n",
    "                groups[identifier][sensor_type] = image_bytes\n",
    "            else:\n",
    "                print(f\"Warning: Unrecognized sensor type '{sensor_type}' in file {file_path}\")\n",
    "\n",
    "print(f\"Total groups collected: {len(groups)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 14111\n"
     ]
    }
   ],
   "source": [
    "records = list(groups.values())\n",
    "print(f\"Total records: {len(records)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset into HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving sensor images: 100%|██████████| 14111/14111 [01:01<00:00, 230.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ../dataset/dataset_ca_17.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Extract metadata columns\n",
    "points     = [rec[\"POINT\"] for rec in records]\n",
    "dates      = [rec[\"DATE\"] for rec in records]\n",
    "regions    = [rec[\"REGION\"] for rec in records]\n",
    "crop_types = [rec[\"CROP_TYPE\"] for rec in records]\n",
    "labels     = [rec[\"LABEL\"] for rec in records]\n",
    "\n",
    "# Helper function: Convert image bytes to uint8 numpy array (or empty array if None)\n",
    "def convert_image_bytes(img_bytes):\n",
    "    if img_bytes is None:\n",
    "        return np.array([], dtype='uint8')\n",
    "    return np.frombuffer(img_bytes, dtype='uint8')\n",
    "\n",
    "# Convert sensor images\n",
    "rgb_images    = [convert_image_bytes(rec[\"RGB\"])    for rec in records]\n",
    "ndvi_images   = [convert_image_bytes(rec[\"NDVI\"])   for rec in records]\n",
    "ndvi45_images = [convert_image_bytes(rec[\"NDVI45\"]) for rec in records]\n",
    "osavi_images  = [convert_image_bytes(rec[\"OSAVI\"])  for rec in records]\n",
    "psri_images   = [convert_image_bytes(rec[\"PSRI\"])   for rec in records]\n",
    "gndvi_images  = [convert_image_bytes(rec[\"GNDVI\"])  for rec in records]\n",
    "\n",
    "with h5py.File(hdf5_path, \"w\") as hf:\n",
    "    # Create datasets for metadata\n",
    "    hf.create_dataset(\"POINT\", data=np.array(points, dtype=np.int64))\n",
    "    hf.create_dataset(\"DATE\", data=np.array(dates, dtype=h5py.string_dtype(encoding='utf-8')))\n",
    "    hf.create_dataset(\"REGION\", data=np.array(regions, dtype=h5py.string_dtype(encoding='utf-8')))\n",
    "    hf.create_dataset(\"CROP_TYPE\", data=np.array(crop_types, dtype=h5py.string_dtype(encoding='utf-8')))\n",
    "    hf.create_dataset(\"LABEL\", data=np.array(labels, dtype=np.int64))\n",
    "    \n",
    "    # Create variable-length datasets for sensor images using uint8 arrays\n",
    "    vlen_uint8 = h5py.special_dtype(vlen=np.dtype('uint8'))\n",
    "    hf.create_dataset(\"RGB\",    (len(rgb_images),),    dtype=vlen_uint8)\n",
    "    hf.create_dataset(\"NDVI\",   (len(ndvi_images),),   dtype=vlen_uint8)\n",
    "    hf.create_dataset(\"NDVI45\", (len(ndvi45_images),), dtype=vlen_uint8)\n",
    "    hf.create_dataset(\"OSAVI\",  (len(osavi_images),),  dtype=vlen_uint8)\n",
    "    hf.create_dataset(\"PSRI\",   (len(psri_images),),   dtype=vlen_uint8)\n",
    "    hf.create_dataset(\"GNDVI\",  (len(gndvi_images),),  dtype=vlen_uint8)\n",
    "    \n",
    "    # Write sensor image data row by row\n",
    "    for i in tqdm(range(len(records)), desc=\"Saving sensor images\"):\n",
    "        hf[\"RGB\"][i]    = rgb_images[i]\n",
    "        hf[\"NDVI\"][i]   = ndvi_images[i]\n",
    "        hf[\"NDVI45\"][i] = ndvi45_images[i]\n",
    "        hf[\"OSAVI\"][i]  = osavi_images[i]\n",
    "        hf[\"PSRI\"][i]   = psri_images[i]\n",
    "        hf[\"GNDVI\"][i]  = gndvi_images[i]\n",
    "\n",
    "print(f\"Data saved to {hdf5_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify HDF5 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets in the file:\n",
      "CROP_TYPE: (14111,)\n",
      "DATE: (14111,)\n",
      "GNDVI: (14111,)\n",
      "LABEL: (14111,)\n",
      "NDVI: (14111,)\n",
      "NDVI45: (14111,)\n",
      "OSAVI: (14111,)\n",
      "POINT: (14111,)\n",
      "PSRI: (14111,)\n",
      "REGION: (14111,)\n",
      "RGB: (14111,)\n"
     ]
    }
   ],
   "source": [
    "# Open the file and print out the keys\n",
    "with h5py.File(hdf5_path, \"r\") as hf:\n",
    "    print(\"Datasets in the file:\")\n",
    "    for key in hf.keys():\n",
    "        print(f\"{key}: {hf[key].shape}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMz4qbCTf4SgqZeC50GYS41",
   "mount_file_id": "1O2z0KrJuca8mLrY2PvlYbmyV7PeSywM3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
